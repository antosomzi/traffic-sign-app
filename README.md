
# Traffic Sign ML Pipeline

> **â„¹ï¸ All deployment, maintenance, and CI/CD commands and processes are documented in detail in [`DEPLOYMENT.md`](DEPLOYMENT.md). Please refer to that file for any production setup, server management, or automation instructions.**

Web application for uploading, validating, and asynchronously processing traffic sign recordings through a machine learning pipeline.

## ğŸ“‘ Table of Contents

- [Architecture](#-architecture)
- [Data Flow](#-flow)
- [Project Structure](#-project-structure)
- [File Storage Structure](#-file-storage-structure)
- [Installation](#-installation)
- [Usage](#-usage)
- [Expected Input Data Structure](#-expected-input-data-structure)
- [Configuration](#-configuration)
- [Security](#-security)

## ğŸ—ï¸ Architecture

- **Flask**: Web interface for file upload, validation, and result delivery
- **Redis**: 
  - Message broker for Celery task queue
  - Shared state storage for extraction progress (critical for multi-worker Gunicorn)
- **Celery**: Asynchronous worker for ML pipeline processing
- **Gunicorn**: Production WSGI server with 4 worker processes

### Multi-Worker Architecture

Production uses **Gunicorn with 4 workers**. Since each worker has separate memory, Redis ensures extraction progress is shared across all workers:

```
Request 1 (Upload) â†’ Worker #1 â†’ Stores progress in Redis
Request 2 (Status) â†’ Worker #3 â†’ Reads progress from Redis âœ…
Request 3 (Status) â†’ Worker #2 â†’ Reads progress from Redis âœ…
```

Without Redis, status checks would return 404 when handled by different workers.

### Execution Modes

Toggle via `USE_GPU_INSTANCE` environment variable:
- **Local mode** (default): Runs pipeline on the same instance
- **GPU mode**: Launches AWS EC2 GPU instance, executes via SSH, auto-stops after completion

For detailed deployment and GPU configuration, see **[DEPLOYMENT.md](DEPLOYMENT.md)**.

## âœ¨ FLow

1. User uploads file â†’ extraction job starts (tracked by `job_id`)
2. Extraction completes â†’ status set to `done` in Redis
3. Celery task is queued for ML pipeline (using `recording_id`)
4. Celery worker processes the task asynchronously
5. Pipeline results and status are written to the recording folder
6. User can download results or check status via the web interface

*See [`JOB_QUEUE_STATUS.md`](JOB_QUEUE_STATUS.md) for a detailed explanation of the job queue, status tracking, and Celery integration.*

## ğŸ“ Project Structure

```
app/
â”œâ”€â”€ app.py                      # Flask application entry point 
â”œâ”€â”€ config.py                   # Centralized configuration management
â”œâ”€â”€ celery_app.py               # Celery configuration
â”œâ”€â”€ tasks.py                    # Async pipeline tasks
â”œâ”€â”€ gpu_pipeline_runner.py      # GPU instance pipeline execution
â”œâ”€â”€ gpu_config.py               # AWS GPU configuration
â”œâ”€â”€ simulate_pipeline.sh        # Pipeline simulation script
â”œâ”€â”€ start_gunicorn.sh           # Production server startup
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .env                        # Environment variables
â”œâ”€â”€ DEPLOYMENT.md               # EC2 deployment guide
â”œâ”€â”€ EC2_GPU_CONFIG.md           # GPU instance setup
â”œâ”€â”€ routes/                     # Blueprint-based routing
â”‚   â”œâ”€â”€ upload_routes.py       # Upload & extraction endpoints
â”‚   â”œâ”€â”€ status_routes.py       # Status monitoring
â”‚   â””â”€â”€ download_routes.py     # Result downloads
â”œâ”€â”€ services/                   # Business logic layer
â”‚   â”œâ”€â”€ redis_service.py       # Redis operations
â”‚   â”œâ”€â”€ validation_service.py  # Structure validation
â”‚   â””â”€â”€ extraction_service.py  # ZIP extraction logic
â”œâ”€â”€ utils/                      # Utility functions
â”‚   â”œâ”€â”€ file_utils.py          # File operations
â”‚   â””â”€â”€ cleanup_utils.py       # macOS file cleanup
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ upload.html            # Upload interface
â”‚   â””â”€â”€ status.html            # Status monitoring
â”œâ”€â”€ recordings/                 # Validated recordings
â”œâ”€â”€ uploads/                    # Uploaded files
â””â”€â”€ temp_extracts/             # Temporary extraction folder
```

## ï¿½ File Storage Structure

**Production paths** (on EC2 main instance):
```
/home/ec2-user/
â”œâ”€â”€ uploads/              # Uploaded ZIP files (persistent storage)
â”‚   â””â”€â”€ <uuid>_<recording_id>.zip
â”œâ”€â”€ recordings/           # Extracted and validated recordings
â”‚   â””â”€â”€ <recording_id>/
â”‚       â”œâ”€â”€ status.json   # Processing status tracking
â”‚       â”œâ”€â”€ result_pipeline_stable/  # ML pipeline outputs
â”‚       â””â”€â”€ <device_id>/  # Original recording data
â”œâ”€â”€ temp_extracts/        # Temporary extraction during validation
â”‚   â””â”€â”€ <job_id>/         # Cleaned up after validation
â””â”€â”€ app/                  # Application files
```

**Local development paths**:
```
app/
â”œâ”€â”€ uploads/              # Uploaded ZIP files
â”œâ”€â”€ recordings/           # Validated recordings
â””â”€â”€ temp_extracts/        # Temporary extraction
```

**Storage notes:**
- `uploads/` folder stores all uploaded ZIP files until manually deleted
- EFS-mounted filesystem (`/home/ec2-user`) enables GPU instance to access recordings
- `.gitignore` excludes all data folders from version control
- Automatic cleanup removes `__MACOSX/`, `.DS_Store`, `._*` files during extraction

## ï¿½ğŸš€ Installation

### Prerequisites

- Python 3.11+
- Redis 6+
- Git

### Local Setup

**1. Clone the repository**
```bash
git clone <repository-url>
cd app
```

**2. Create virtual environment**
```bash
python3 -m venv venv
source venv/bin/activate
```

**3. Install dependencies**
```bash
pip install -r requirements.txt
```

**4. Install and start Redis**

macOS (Homebrew):
```bash
brew install redis
redis-server
```

Ubuntu/Debian:
```bash
sudo apt-get install redis-server
sudo systemctl start redis
```

**5. Make scripts executable**
```bash
chmod +x simulate_pipeline.sh start_gunicorn.sh
```

### Running Locally (Development)

Open **3 terminals**:

```bash
# Terminal 1 - Redis
redis-server

# Terminal 2 - Celery Worker
celery -A tasks worker --loglevel=INFO

# Terminal 3 - Flask App
python app.py
```

Access the application at: **http://localhost:5000**

## ğŸ“ Usage

### 1. Upload Recording

1. Navigate to `http://localhost:5000`
2. Drag-and-drop or select a folder containing your `.mp4` and GPS `.csv` files, organized in the required structure (see below).
3. The folder will be automatically zipped client-side (store mode, equivalent to `zip -0`) before being sent to the server.
4. Click "Upload and Validate".
5. The system will:
  - Extract the received zip
  - Validate the folder structure
  - Launch the pipeline if the structure is correct

### 2. Monitor Processing

- Click "View Recording Status"
- Track all recordings and their pipeline progress
- Page auto-refreshes every 10 seconds

### 3. Download Results

- Once processing is complete, click "Download Results"
- Downloads a ZIP containing `supports.csv` and `signs.csv`

## ğŸ—‚ï¸ Expected Input Data Structure


The uploaded folder (which will be zipped client-side) must contain exactly one root folder with the following minimal structure:

```
recording_id/
  â””â”€â”€ device_id/
      â””â”€â”€ imei_folder/
          â”œâ”€â”€ camera/
          â”‚     â””â”€â”€ <video_file>.mp4
          â””â”€â”€ location/
                â”œâ”€â”€ <file1>.csv
                â””â”€â”€ <file2>.csv
```

**Notes:**
- macOS system files (`__MACOSX/`, `.DS_Store`, `._*`) are automatically removed
- Structure validation is strict â€“ only the above folders/files are required

## âš™ï¸ Configuration

### Environment Variables

Create a `.env` file (optional for local development):

```bash
# Redis password (required for production)
REDIS_PASSWORD=your_password_here

# Execution mode (local or GPU instance)
USE_GPU_INSTANCE=false

# Flask environment
FLASK_ENV=production
```

## ï¿½ Security

- **ZipSlip protection**: Validates file paths during extraction
- **Strict validation**: Enforces expected folder structure
- **File size limit**: 8 GB maximum
- **Allowed formats**: ZIP, TAR, TAR.GZ, TGZ
- **Automatic cleanup**: Removes partial uploads on validation failure
- **Redis authentication**: Required for production environments

